{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rGFaa-tzTtZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and pre-processing without brightness adjustment"
      ],
      "metadata": {
        "id": "dKiu6X71hdlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading the csv"
      ],
      "metadata": {
        "id": "xItO1NqSh_7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#data = pd.read_csv('/content/drive/MyDrive/RAFdb/dataset2.csv')\n",
        "data = pd.read_csv('/content/drive/MyDrive/RAFdb/rafDB.csv')\n",
        "images = data['pixels']\n",
        "labels = data['emotion']"
      ],
      "metadata": {
        "id": "itLmzkp7hytm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocssing\n",
        "\n"
      ],
      "metadata": {
        "id": "pwWcPUz2iJEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(images):\n",
        "   images = np.array([np.fromstring(image, dtype=int, sep=' ').reshape(48, 48) for image in images])\n",
        "   images = np.expand_dims(images, axis=-1)\n",
        "   images = images / 255.0\n",
        "   return images\n",
        "\n",
        "images = preprocess_images(images)"
      ],
      "metadata": {
        "id": "xqu-tCSDitJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Dataset"
      ],
      "metadata": {
        "id": "hlHQylbZlQ7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22, random_state=42)"
      ],
      "metadata": {
        "id": "q8ngZPkflPe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the CNN Model"
      ],
      "metadata": {
        "id": "7gzSAFlllWV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Reshape((48, 48), input_shape=(48, 48, 1)),\n",
        "    layers.GRU(256, return_sequences=True),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GRU(128, return_sequences=True),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GRU(64),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(7, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "pXbRAPIolU-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and Train the Model"
      ],
      "metadata": {
        "id": "n6OvM4p8lgy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YuafFRNlcrg",
        "outputId": "cc68b917-85c5-48c0-8b04-412ce1ce4f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/24\n",
            "267/267 [==============================] - 23s 26ms/step - loss: 2.0642 - accuracy: 0.3353 - val_loss: 1.7116 - val_accuracy: 0.4919\n",
            "Epoch 2/24\n",
            "267/267 [==============================] - 5s 20ms/step - loss: 1.4629 - accuracy: 0.4968 - val_loss: 1.3364 - val_accuracy: 0.5223\n",
            "Epoch 3/24\n",
            "267/267 [==============================] - 4s 13ms/step - loss: 1.3163 - accuracy: 0.5331 - val_loss: 1.2289 - val_accuracy: 0.5452\n",
            "Epoch 4/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 1.2393 - accuracy: 0.5532 - val_loss: 1.1850 - val_accuracy: 0.5655\n",
            "Epoch 5/24\n",
            "267/267 [==============================] - 5s 19ms/step - loss: 1.1926 - accuracy: 0.5650 - val_loss: 1.1486 - val_accuracy: 0.5685\n",
            "Epoch 6/24\n",
            "267/267 [==============================] - 6s 22ms/step - loss: 1.1605 - accuracy: 0.5822 - val_loss: 1.1726 - val_accuracy: 0.5622\n",
            "Epoch 7/24\n",
            "267/267 [==============================] - 6s 23ms/step - loss: 1.1392 - accuracy: 0.5861 - val_loss: 1.2663 - val_accuracy: 0.4919\n",
            "Epoch 8/24\n",
            "267/267 [==============================] - 4s 13ms/step - loss: 1.1090 - accuracy: 0.5915 - val_loss: 1.4348 - val_accuracy: 0.5364\n",
            "Epoch 9/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 1.0864 - accuracy: 0.6048 - val_loss: 1.0685 - val_accuracy: 0.6109\n",
            "Epoch 10/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 1.0499 - accuracy: 0.6237 - val_loss: 1.1215 - val_accuracy: 0.6109\n",
            "Epoch 11/24\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 1.0144 - accuracy: 0.6411 - val_loss: 1.0389 - val_accuracy: 0.6359\n",
            "Epoch 12/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.9779 - accuracy: 0.6502 - val_loss: 0.9961 - val_accuracy: 0.6296\n",
            "Epoch 13/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.9425 - accuracy: 0.6647 - val_loss: 1.0098 - val_accuracy: 0.6280\n",
            "Epoch 14/24\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 0.8991 - accuracy: 0.6824 - val_loss: 0.9928 - val_accuracy: 0.6529\n",
            "Epoch 15/24\n",
            "267/267 [==============================] - 3s 13ms/step - loss: 0.8756 - accuracy: 0.7004 - val_loss: 0.9602 - val_accuracy: 0.6754\n",
            "Epoch 16/24\n",
            "267/267 [==============================] - 3s 13ms/step - loss: 0.8343 - accuracy: 0.7127 - val_loss: 0.9144 - val_accuracy: 0.6675\n",
            "Epoch 17/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.8021 - accuracy: 0.7219 - val_loss: 0.9527 - val_accuracy: 0.6779\n",
            "Epoch 18/24\n",
            "267/267 [==============================] - 4s 16ms/step - loss: 0.7712 - accuracy: 0.7350 - val_loss: 0.8962 - val_accuracy: 0.7066\n",
            "Epoch 19/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.7470 - accuracy: 0.7445 - val_loss: 0.9279 - val_accuracy: 0.6567\n",
            "Epoch 20/24\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 0.7237 - accuracy: 0.7550 - val_loss: 0.9414 - val_accuracy: 0.6762\n",
            "Epoch 21/24\n",
            "267/267 [==============================] - 5s 20ms/step - loss: 0.6849 - accuracy: 0.7629 - val_loss: 0.8292 - val_accuracy: 0.7091\n",
            "Epoch 22/24\n",
            "267/267 [==============================] - 3s 13ms/step - loss: 0.6540 - accuracy: 0.7829 - val_loss: 0.8699 - val_accuracy: 0.7066\n",
            "Epoch 23/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.6443 - accuracy: 0.7781 - val_loss: 0.9551 - val_accuracy: 0.6916\n",
            "Epoch 24/24\n",
            "267/267 [==============================] - 5s 20ms/step - loss: 0.6261 - accuracy: 0.7901 - val_loss: 0.8290 - val_accuracy: 0.7295\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb58ed84c40>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and pre-processing with brightness adjustment"
      ],
      "metadata": {
        "id": "e8XxRKRRddDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/RAFdb/rafDB.csv')\n",
        "images = data['pixels']\n",
        "labels = data['emotion']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "3kyg3k71eECF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(images):\n",
        "    images = np.array([np.fromstring(image, dtype=int, sep=' ').reshape(48, 48) for image in images])\n",
        "    images = np.expand_dims(images, axis=-1)\n",
        "    images = images / 255.0\n",
        "    brightness = tf.random.uniform([], -0.5, 0.5)\n",
        "    images = tf.image.adjust_brightness(images, delta=brightness)\n",
        "    return images.numpy() # convert Tensor object back to numpy array\n",
        "\n",
        "images = preprocess_images(images)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "8HRCcBOYd8uR",
        "outputId": "f6b890a0-a9fa-4fbe-a9d9-9ee84e8100a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0f24a2ebca10>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert Tensor object back to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22, random_state=42)"
      ],
      "metadata": {
        "id": "4DIqyuYBnyqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Reshape((48, 48), input_shape=(48, 48, 1)),\n",
        "    layers.GRU(256, return_sequences=True),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GRU(128, return_sequences=True),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.GRU(64),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(7, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "dI277o2En26-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akmerd5Un7AX",
        "outputId": "6356fb7f-0b40-4d60-8f1f-ddb298989a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/24\n",
            "267/267 [==============================] - 18s 21ms/step - loss: 2.1049 - accuracy: 0.3241 - val_loss: 1.8715 - val_accuracy: 0.4806\n",
            "Epoch 2/24\n",
            "267/267 [==============================] - 4s 13ms/step - loss: 1.5226 - accuracy: 0.4776 - val_loss: 1.7112 - val_accuracy: 0.4969\n",
            "Epoch 3/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 1.3362 - accuracy: 0.5279 - val_loss: 1.2545 - val_accuracy: 0.5568\n",
            "Epoch 4/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 1.2528 - accuracy: 0.5546 - val_loss: 1.1980 - val_accuracy: 0.5618\n",
            "Epoch 5/24\n",
            "267/267 [==============================] - 4s 16ms/step - loss: 1.1987 - accuracy: 0.5704 - val_loss: 1.1591 - val_accuracy: 0.5830\n",
            "Epoch 6/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 1.1683 - accuracy: 0.5818 - val_loss: 1.2628 - val_accuracy: 0.5472\n",
            "Epoch 7/24\n",
            "267/267 [==============================] - 4s 13ms/step - loss: 1.1343 - accuracy: 0.5891 - val_loss: 1.1349 - val_accuracy: 0.5951\n",
            "Epoch 8/24\n",
            "267/267 [==============================] - 4s 13ms/step - loss: 1.0963 - accuracy: 0.6033 - val_loss: 1.1996 - val_accuracy: 0.5556\n",
            "Epoch 9/24\n",
            "267/267 [==============================] - 4s 15ms/step - loss: 1.0736 - accuracy: 0.6186 - val_loss: 1.2287 - val_accuracy: 0.5422\n",
            "Epoch 10/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 1.0307 - accuracy: 0.6321 - val_loss: 1.0941 - val_accuracy: 0.5872\n",
            "Epoch 11/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 0.9815 - accuracy: 0.6539 - val_loss: 1.2997 - val_accuracy: 0.5489\n",
            "Epoch 12/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.9523 - accuracy: 0.6688 - val_loss: 1.0394 - val_accuracy: 0.6055\n",
            "Epoch 13/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.9102 - accuracy: 0.6824 - val_loss: 0.9934 - val_accuracy: 0.6325\n",
            "Epoch 14/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.8720 - accuracy: 0.6972 - val_loss: 0.8798 - val_accuracy: 0.6775\n",
            "Epoch 15/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 0.8568 - accuracy: 0.6971 - val_loss: 0.9200 - val_accuracy: 0.6816\n",
            "Epoch 16/24\n",
            "267/267 [==============================] - 4s 16ms/step - loss: 0.8318 - accuracy: 0.7084 - val_loss: 1.2263 - val_accuracy: 0.5435\n",
            "Epoch 17/24\n",
            "267/267 [==============================] - 3s 13ms/step - loss: 0.7980 - accuracy: 0.7200 - val_loss: 1.0231 - val_accuracy: 0.6783\n",
            "Epoch 18/24\n",
            "267/267 [==============================] - 3s 13ms/step - loss: 0.7795 - accuracy: 0.7322 - val_loss: 0.8387 - val_accuracy: 0.7062\n",
            "Epoch 19/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 0.7539 - accuracy: 0.7342 - val_loss: 0.9217 - val_accuracy: 0.6775\n",
            "Epoch 20/24\n",
            "267/267 [==============================] - 6s 22ms/step - loss: 0.7490 - accuracy: 0.7427 - val_loss: 0.8971 - val_accuracy: 0.6754\n",
            "Epoch 21/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 0.7154 - accuracy: 0.7555 - val_loss: 0.8462 - val_accuracy: 0.7087\n",
            "Epoch 22/24\n",
            "267/267 [==============================] - 3s 12ms/step - loss: 0.7143 - accuracy: 0.7568 - val_loss: 0.9464 - val_accuracy: 0.6663\n",
            "Epoch 23/24\n",
            "267/267 [==============================] - 4s 15ms/step - loss: 0.6846 - accuracy: 0.7643 - val_loss: 0.8187 - val_accuracy: 0.7179\n",
            "Epoch 24/24\n",
            "267/267 [==============================] - 4s 14ms/step - loss: 0.6562 - accuracy: 0.7712 - val_loss: 1.0410 - val_accuracy: 0.6667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f06f1df0fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}